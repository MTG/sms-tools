{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A10-2 Sound and music description, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "(This assignment needs a working installation of Essentia library and requires good amount of independent programming.)\n",
    "In this assignment, you will extend the sound and music description task you did in A9 to a larger set of instruments and explore possible improvements in a task of Instrument identification from single note/stroke sounds. By doing this assignment, you will get hands on experience with Essentia and better insights into complexities arising in a real world Music Information Retrieval problem, with a larger set of descriptors and more instrument classes. You will present the results and findings as a short report.\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "In A9, you explored the tasks of clustering and classification with sound excerpts of three instru- ments. Typically, as we add more instruments for clustering, the average performance degrades. In such situations, clustering performance can be improved in several ways, two of which you will explore in this assignment:\n",
    "- Improving descriptor selection: We can use a better set of descriptors, in addition to the ones you used in A9. To improve performance, we also need to typically increase the number of descriptors used for clustering as the number of instrument classes increase.\n",
    "- Improving descriptor computation: In A9, each descriptor you used is a time averaged mean of the feature computed over short frames of the audio file. However, there are segments in an audio file (typically at the beginning and the end) where there is silence or low energy background noise. Such segments should be discarded while computing the global statistics of the descriptors, e.g., low energy regions (silence) have a higher spectral centroid and affect the average spectral centroid adversely if included in computing the average.\n",
    "\n",
    "You will use Essentia to implement both these improvements. You can use the scripts provided with A9 as a base (get A9 scripts) and build your code using them. You first need to install Essentia to compute some of the descriptors that you will be exploring for the task. You can find the download and install instructions for Essentia here: http://essentia.upf.edu/. Essentia has extensive documentation that will be useful in this assignment http://essentia.upf.edu/ documentation/index.html.\n",
    "\n",
    "The questions in the assignment have been presented separately for the ease of evaluation. But you will write the answers to all the questions together in a document and upload your report (PDF, 2 pages max., excluding plots/illustrations/parameter listings) in Question 1. You must also upload the code that you write. You will evaluate a minimum of three other peers in this assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 1: Downloading sounds\n",
    "\n",
    "Choose at least 10 different instrumental sounds classes from the following possible classes: violin, guitar, bassoon, trumpet, clarinet, cello, naobo, snare drum, flute, mridangam, daluo, xiaoluo. For each instrument class, use soundDownload.py script from A9 to download the audio and\n",
    "descriptors of 20 examples of representative single notes/strokes of each instrument. Since you will use the sounds also to extract descriptors using Essentia, make sure you download the high quality mp3 and not the low quality mp3 preview. To achieve this in the soundDownload.py script, you can replace fs.FSRequest.retrieve(sound.previews.preview lq mp3, fsClnt, mp3Path) to fs.FSRequest.retrieve(sound.previews.preview hq mp3, fsClnt, mp3Path)\n",
    "In the report, explain your choices, query text and the tags you used. Include the Freesound links to the downloaded sounds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import soundDownload as SD\n",
    "\n",
    "API_key = 'OSq4Ic5Wg4zdVdTuhNZaiT4kKlKzsnnFvIIzU6p6'\n",
    "\n",
    "instruments_lst = ['violin', 'guitar', 'cello', 'clarinet', 'flue', 'bassoon', 'trumpet',  'naobo', 'snare drum',  'daluo']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the violin\n",
    "SD.downloadSoundsFreesound(queryText='violin',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0.01, 10), tag=\"(single-note AND strings)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the cello\n",
    "SD.downloadSoundsFreesound(queryText='cello',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0.01, 10), tag=\"(single-note AND strings)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the guitar\n",
    "SD.downloadSoundsFreesound(queryText='guitar',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0.01, 10), tag=\"(string AND simplesamples)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the clarinet\n",
    "SD.downloadSoundsFreesound(queryText='clarinet',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0.03, 10), tag=\"(single-note AND clarinet AND multisample)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the flute\n",
    "SD.downloadSoundsFreesound(queryText='flute',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(6, 10), tag=\"(single-note AND flute AND good-sounds \"\n",
    "                                                    \"AND multisample AND neumann-U87)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the bassoon\n",
    "SD.downloadSoundsFreesound(queryText='bassoon',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0, 10), tag=\"(single-note)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the trumpet\n",
    "SD.downloadSoundsFreesound(queryText='trumpet',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(4, 8), tag=\"(single-note AND multisample AND trumpet AND neumann-u87)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the naobo\n",
    "SD.downloadSoundsFreesound(queryText='naobo',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0.01, 6.5), tag=\"(beijing-opera AND chinese AND icassp2014-dataset)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the snare drum\n",
    "SD.downloadSoundsFreesound(queryText='snaredrum',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0, 2), tag=\"(snare AND drum AND 1-shot AND velocity AND multisample)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download the daluo\n",
    "SD.downloadSoundsFreesound(queryText='daluo',\n",
    "                           API_Key=API_key, outputDir='test_download/', topNResults=20,\n",
    "                           duration=(0, 9), tag=\"(beijing-opera AND chinese AND icassp2014-dataset AND qmul AND compmusic)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 2: Obtaining a baseline clustering performance\n",
    "\n",
    "Visualize different pairs of descriptors and choose a subset of the descriptors you downloaded along with the audio (same as A9) for a good separation between classes. Run a k-means clustering task with the 10 instrument dataset using the chosen subset of descriptors. You can use the soundAnalysis.py script from A9 for this task. Use the same number of clusters as the number of different instruments.\n",
    "\n",
    "Report the subset of descriptors used and the clustering accuracy you obtained. Since k-means algorithm is randomly initiated and gives a different result every time it is run, report the average performance over 10 runs of the algorithm. This performance result acts as your baseline, over which you will improve in Question 3.\n",
    "\n",
    "Obtaining a baseline performance is necessary to suggest and evaluate improvements. For the 10 instrument class problem, the random baseline is 10% (randomly choosing one out of the ten classes). But as you will see, the baseline you obtain will be higher that 10%, but lower than that you obtained for three instruments in A9 (with a careful selection of descriptors).\n",
    "You will upload a single PDF file containing a report answering all questions of this assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import soundAnalysis as SA\n",
    "SA.descriptorPairScatterPlot('test_download/', descInput=(2,12,14))\n",
    "SA.showDescriptorMapping()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os, sys\n",
    "import json\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "\n",
    "# from scipy.stats import mode\n",
    "\n",
    "# Mapping of descriptors\n",
    "descriptorMapping = {0: 'lowlevel.spectral_centroid.mean',\n",
    "                     1: 'lowlevel.dissonance.mean',\n",
    "                     2: 'lowlevel.hfc.mean',\n",
    "                     3: 'sfx.logattacktime.mean',\n",
    "                     4: 'sfx.inharmonicity.mean',\n",
    "                     5: 'lowlevel.spectral_contrast.mean.0',\n",
    "                     6: 'lowlevel.spectral_contrast.mean.1',\n",
    "                     7: 'lowlevel.spectral_contrast.mean.2',\n",
    "                     8: 'lowlevel.spectral_contrast.mean.3',\n",
    "                     9: 'lowlevel.spectral_contrast.mean.4',\n",
    "                     10: 'lowlevel.spectral_contrast.mean.5',\n",
    "                     11: 'lowlevel.mfcc.mean.0',\n",
    "                     12: 'lowlevel.mfcc.mean.1',\n",
    "                     13: 'lowlevel.mfcc.mean.2',\n",
    "                     14: 'lowlevel.mfcc.mean.3',\n",
    "                     15: 'lowlevel.mfcc.mean.4',\n",
    "                     16: 'lowlevel.mfcc.mean.5'\n",
    "                     }\n",
    "\n",
    "\n",
    "def showDescriptorMapping():\n",
    "    \"\"\"\n",
    "  This function prints the mapping of integers to sound descriptors.\n",
    "  \"\"\"\n",
    "    for key in descriptorMapping.keys():\n",
    "        print (\"Number %d is for %s\" % (key, descriptorMapping[key]))\n",
    "\n",
    "\n",
    "def descriptorPairScatterPlot(inputDir, descInput=(0, 0), anotOn=0):\n",
    "    \"\"\"\n",
    "  This function does a scatter plot of the chosen feature pairs for all the sounds in the\n",
    "  directory inputDir. The chosen features are specified in descInput as a tuple.\n",
    "  Additionally, you can annotate the sound id on the scatter plot by setting anotOn = 1\n",
    "\n",
    "  Input:\n",
    "    inputDir (string): path to the directory where the sound samples and descriptors are present\n",
    "    descInput (tuple): pair of descriptor indices (see descriptorMapping for mapping between\n",
    "                       indices and descriptor names)\n",
    "    anotOn (int): Set this flag to 1 to annotate the scatter points with the sound id. (Default = 0)\n",
    "\n",
    "  Output:\n",
    "    scatter plot of the chosen pair of descriptors for all the sounds in the directory inputDir\n",
    "  \"\"\"\n",
    "    if max(descInput) >= len(descriptorMapping.keys()):\n",
    "        print(\"Please select a descriptor index that is within the range. Maximum descriptor index can be \" +\n",
    "              str(len(descriptorMapping) - 1) + \". Check the descriptor index mapping again using function \"\n",
    "                                                \"showDescriptorMapping().\")\n",
    "\n",
    "    dataDetails = fetchDataDetails(inputDir)\n",
    "    colors = ['r', 'g', 'c', 'b', 'k', 'm', 'y']\n",
    "    plt.figure()\n",
    "    plt.hold(True)\n",
    "    legArray = []\n",
    "    catArray = []\n",
    "    for ii, category in enumerate(dataDetails.keys()):\n",
    "        catArray.append(category)\n",
    "        for soundId in dataDetails[category].keys():\n",
    "            filepath = os.path.join(inputDir, category, soundId, dataDetails[category][soundId]['file'])\n",
    "            descSound = convFtrDict2List(json.load(open(filepath, 'r')))\n",
    "            x_cord = descSound[descInput[0]]\n",
    "            y_cord = descSound[descInput[1]]\n",
    "\n",
    "            plt.scatter(x_cord, y_cord, c=colors[ii], s=200, hold=True, alpha=0.75)\n",
    "            if anotOn == 1:\n",
    "                plt.annotate(soundId, xy=(x_cord, y_cord), xytext=(x_cord, y_cord))\n",
    "\n",
    "        circ = Line2D([0], [0], linestyle=\"none\", marker=\"o\", alpha=0.75, markersize=10, markerfacecolor=colors[ii])\n",
    "        legArray.append(circ)\n",
    "\n",
    "    plt.ylabel(descriptorMapping[descInput[1]], fontsize=16)\n",
    "    plt.xlabel(descriptorMapping[descInput[0]], fontsize=16)\n",
    "    plt.legend(legArray, catArray, numpoints=1, bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=len(catArray),\n",
    "               mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def convFtrDict2List(ftrDict):\n",
    "    \"\"\"\n",
    "  This function converts descriptor dictionary to an np.array. The order in the numpy array (indices)\n",
    "  are same as those mentioned in descriptorMapping dictionary.\n",
    "\n",
    "  Input:\n",
    "    ftrDict (dict): dictionary containing descriptors downloaded from the freesound\n",
    "  Output:\n",
    "    ftr (np.ndarray): Numpy array containing the descriptors for processing later on\n",
    "  \"\"\"\n",
    "    ftr = []\n",
    "    for key in range(len(descriptorMapping.keys())):\n",
    "        try:\n",
    "            ftrName, ind = '.'.join(descriptorMapping[key].split('.')[:-1]), int(descriptorMapping[key].split('.')[-1])\n",
    "            ftr.append(ftrDict[ftrName][0][ind])\n",
    "        except:\n",
    "            ftr.append(ftrDict[descriptorMapping[key]][0])\n",
    "    return np.array(ftr)\n",
    "\n",
    "\n",
    "def computeSimilarSounds(queryFile, targetDir, descInput=[]):\n",
    "    \"\"\"\n",
    "  This function returns similar sounds for a specific queryFile. Given a queryFile this function\n",
    "  computes the distance of the query to all the sounds found in the targetDir and sorts them in\n",
    "  the increasing order of the distance. This way we can obtain similar sounds to a query sound.\n",
    "\n",
    "  Input:\n",
    "    queryFile (string): Descriptor file (.json, unless changed)\n",
    "    targetDir (string): Target directory to search for similar sounds (using their descriptor files)\n",
    "    descInput (list) : list of indices of the descriptors to be used for similarity/distance computation\n",
    "                       (see descriptorMapping)\n",
    "  Output:\n",
    "    List containing an ordered list of similar sounds.\n",
    "  \"\"\"\n",
    "\n",
    "    dataDetails = fetchDataDetails(targetDir)\n",
    "\n",
    "    # reading query feature dictionary\n",
    "    qFtr = json.load(open(queryFile, 'r'))\n",
    "\n",
    "    dist = []\n",
    "    # Iterating over classes\n",
    "    for cname in dataDetails.keys():\n",
    "        # Iterating over sounds\n",
    "        for sname in dataDetails[cname].keys():\n",
    "            eucDist = eucDistFeatures(qFtr, dataDetails[cname][sname]['feature'], descInput)\n",
    "            dist.append([eucDist, sname, cname])\n",
    "\n",
    "    # Sorting the array based on the distance\n",
    "    indSort = np.argsort(np.array(dist)[:, 0])\n",
    "    return (np.array(dist)[indSort, :]).tolist()\n",
    "\n",
    "\n",
    "def classifySoundkNN(queryFile, targetDir, K, descInput=[]):\n",
    "    \"\"\"\n",
    "  This function performs the KNN classification of a sound. The nearest neighbors are chosen from\n",
    "  the sounds in the targetDir.\n",
    "\n",
    "  Input:\n",
    "    queryFile (string): Descriptor file (.json, unless changed)\n",
    "    targetDir (string): Target directory to search for similar sounds (using their descriptor files)\n",
    "    K (int) : Number of nearest neighbors to consider for KNN classification.\n",
    "    descInput (list) : List of indices of the descriptors to be used for similarity/distance computation\n",
    "                      (see descriptorMapping)\n",
    "  Output:\n",
    "    predClass (string): Predicted class of the query sound\n",
    "  \"\"\"\n",
    "    distances = computeSimilarSounds(queryFile, targetDir, descInput)\n",
    "\n",
    "    if len(np.where((np.array(distances)[:, 0].astype(np.float64)) == 0)[0]) > 0:\n",
    "        print(\"Warning: We found an exact copy of the query file in the target directory. \"\n",
    "              \"Beware of duplicates while doing KNN classification.\")\n",
    "\n",
    "    classes = (np.array(distances)[:K, 2]).tolist()\n",
    "    freqCnt = []\n",
    "    for ii in range(K):\n",
    "        freqCnt.append(classes.count(classes[ii]))\n",
    "    indMax = np.argmax(freqCnt)\n",
    "    predClass = classes[indMax]\n",
    "    print (\"This sample belongs to class: \" + str(predClass))\n",
    "    return predClass\n",
    "\n",
    "\n",
    "def clusterSounds(targetDir, nCluster=-1, descInput=[]):\n",
    "    \"\"\"\n",
    "  This function clusters all the sounds in targetDir using kmeans clustering.\n",
    "\n",
    "  Input:\n",
    "    targetDir (string): Directory where sound descriptors are stored (all the sounds in this\n",
    "                        directory will be used for clustering)\n",
    "    nCluster (int): Number of clusters to be used for kmeans clustering.\n",
    "    descInput (list) : List of indices of the descriptors to be used for similarity/distance\n",
    "                       computation (see descriptorMapping)\n",
    "  Output:\n",
    "    Prints the class of each cluster (computed by a majority vote), number of sounds in each\n",
    "    cluster and information (sound-id, sound-class and classification decision) of the sounds\n",
    "    in each cluster. Optionally, you can uncomment the return statement to return the same data.\n",
    "  \"\"\"\n",
    "\n",
    "    dataDetails = fetchDataDetails(targetDir)\n",
    "\n",
    "    ftrArr = []\n",
    "    infoArr = []\n",
    "\n",
    "    if nCluster == -1:\n",
    "        nCluster = len(dataDetails.keys())\n",
    "    for cname in dataDetails.keys():\n",
    "        # iterating over sounds\n",
    "        for sname in dataDetails[cname].keys():\n",
    "            ftrArr.append(convFtrDict2List(dataDetails[cname][sname]['feature'])[descInput])\n",
    "            infoArr.append([sname, cname])\n",
    "\n",
    "    ftrArr = np.array(ftrArr)\n",
    "    infoArr = np.array(infoArr)\n",
    "\n",
    "    ftrArrWhite = whiten(ftrArr)\n",
    "    centroids, distortion = kmeans(ftrArrWhite, nCluster)\n",
    "    clusResults = -1 * np.ones(ftrArrWhite.shape[0])\n",
    "\n",
    "    for ii in range(ftrArrWhite.shape[0]):\n",
    "        diff = centroids - ftrArrWhite[ii, :]\n",
    "        diff = np.sum(np.power(diff, 2), axis=1)\n",
    "        indMin = np.argmin(diff)\n",
    "        clusResults[ii] = indMin\n",
    "\n",
    "    ClusterOut = []\n",
    "    classCluster = []\n",
    "    globalDecisions = []\n",
    "    for ii in range(nCluster):\n",
    "        ind = np.where(clusResults == ii)[0]\n",
    "        freqCnt = []\n",
    "        for elem in infoArr[ind, 1]:\n",
    "            freqCnt.append(infoArr[ind, 1].tolist().count(elem))\n",
    "        indMax = np.argmax(freqCnt)\n",
    "        classCluster.append(infoArr[ind, 1][indMax])\n",
    "\n",
    "        print(\"\\n(Cluster: \" + str(ii) + \") Using majority voting as a criterion this cluster belongs to \" +\n",
    "              \"class: \" + classCluster[-1])\n",
    "        print (\"Number of sounds in this cluster are: \" + str(len(ind)))\n",
    "        decisions = []\n",
    "        for jj in ind:\n",
    "            if infoArr[jj, 1] == classCluster[-1]:\n",
    "                decisions.append(1)\n",
    "            else:\n",
    "                decisions.append(0)\n",
    "        globalDecisions.extend(decisions)\n",
    "        print (\"sound-id, sound-class, classification decision\")\n",
    "        ClusterOut.append(np.hstack((infoArr[ind], np.array([decisions]).T)))\n",
    "        print (ClusterOut[-1])\n",
    "    globalDecisions = np.array(globalDecisions)\n",
    "    totalSounds = len(globalDecisions)\n",
    "    nIncorrectClassified = len(np.where(globalDecisions == 0)[0])\n",
    "    print(\"Out of %d sounds, %d sounds are incorrectly classified considering that one cluster should \"\n",
    "          \"ideally contain sounds from only a single class\" % (totalSounds, nIncorrectClassified))\n",
    "    print(\"You obtain a classification (based on obtained clusters and majority voting) accuracy \"\n",
    "          \"of %.2f percentage\" % round(float(100.0 * float(totalSounds - nIncorrectClassified) / totalSounds), 2))\n",
    "\n",
    "    acc = float(100.0 * float(totalSounds - nIncorrectClassified) / totalSounds)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fetchDataDetails(inputDir, descExt='.json'):\n",
    "    \"\"\"\n",
    "  This function is used by other functions to obtain the information regarding the directory structure\n",
    "  and the location of descriptor files for each sound\n",
    "  \"\"\"\n",
    "    dataDetails = {}\n",
    "    for path, dname, fnames in os.walk(inputDir):\n",
    "        for fname in fnames:\n",
    "            if descExt in fname.lower():\n",
    "                remain, rname, cname, sname = path.split('/')[:-3], path.split('/')[-3], path.split('/')[-2], \\\n",
    "                path.split('/')[-1]\n",
    "                if cname not in dataDetails:\n",
    "                    dataDetails[cname] = {}\n",
    "                fDict = json.load(open(os.path.join('/'.join(remain), rname, cname, sname, fname), 'r'))\n",
    "                dataDetails[cname][sname] = {'file': fname, 'feature': fDict}\n",
    "    return dataDetails\n",
    "\n",
    "\n",
    "def eucDistFeatures(ftrDict1, ftrDict2, ftrInds):\n",
    "    \"\"\"\n",
    "  This function computes Euclidean distance between two descriptor vectors (input as dictionaries).\n",
    "  Additionally, also provide a list of the indices of the descriptor vectors that need to be used\n",
    "  in the distance computation.\n",
    "\n",
    "  Input:\n",
    "    ftrDict1 (dict): Feature vector dictionary 1\n",
    "    ftrDict2 (dict): Feature vector dictionary 2\n",
    "    ftrInds (list): List of indices of descriptor vectors to be used in\n",
    "                    distance computation (see descriptorMapping)\n",
    "  \"\"\"\n",
    "    f1 = convFtrDict2List(ftrDict1)\n",
    "    f2 = convFtrDict2List(ftrDict2)\n",
    "    return eucDist(f1[ftrInds], f2[ftrInds])\n",
    "\n",
    "\n",
    "def eucDist(vec1, vec2):\n",
    "    \"\"\"\n",
    "  Computes the euclidean distance between two vectors\n",
    "  \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(np.array(vec1) - np.array(vec2), 2)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def cluster_sounds_helper(desc_lst):\n",
    "    return clusterSounds('test_download/', nCluster=10, descInput=desc_lst)\n",
    "\n",
    "elements = range(17)\n",
    "k = 15\n",
    "#given_comb = [3] # 48.02\n",
    "#given_comb = [3, 11] # 61.20\n",
    "#given_comb = [3, 11, 13] # 71.70\n",
    "#given_comb = [1, 3, 11, 13] # 75.63\n",
    "#given_comb = [1, 3, 9, 11, 13] # 75.60\n",
    "#given_comb = [1, 3, 9, 11, 13, 16] # 73.90\n",
    "#given_comb = [1, 3, 9, 10, 11, 13, 16] # 72.90\n",
    "#given_comb = [1, 3, 7, 9, 10, 11, 13, 15, 16] # 71.00\n",
    "#given_comb = [1, 3, 6, 7, 9, 10, 11, 13, 15, 16] #69.17\n",
    "#given_comb = [1, 3, 6, 7, 8, 9, 10, 11, 13, 15, 16] #68.27\n",
    "#given_comb = [0, 1, 3, 6, 7, 8, 9, 10, 11, 13, 15, 16]  # 68.23\n",
    "#given_comb = [0, 1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16] # 68.10\n",
    "#given_comb = [0, 1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] # 67.83\n",
    "given_comb = [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] # 66.10\n",
    "\n",
    "combinations = list(itertools.combinations(elements, k))\n",
    "filtered_combinations = [comb for comb in combinations if set(given_comb).issubset(set(comb))]\n",
    "result = filtered_combinations\n",
    "#result = combinations\n",
    "\n",
    "iter = 15\n",
    "max_mean = 0.0\n",
    "best_desc = []\n",
    "des_max_acc = 0.0\n",
    "sum = 0.0\n",
    "\n",
    "# Find the best_desc\n",
    "for desc_list in result:\n",
    "    desc_list = list(desc_list)\n",
    "    acc = cluster_sounds_helper(desc_list)\n",
    "    if acc >= des_max_acc:\n",
    "        best_desc = desc_list\n",
    "        des_max_acc = acc\n",
    "\n",
    "for desc_list in result:\n",
    "    desc_list = list(desc_list)\n",
    "    sum = 0.0\n",
    "    for i in range(iter):\n",
    "        acc = cluster_sounds_helper(desc_list)\n",
    "        sum += acc\n",
    "    mean_acc = float(sum/iter)\n",
    "    if mean_acc >= max_mean:\n",
    "        best_desc = desc_list\n",
    "        max_mean = mean_acc\n",
    "\n",
    "print('**************************** Best Acc. Mean = %.2f ****************************' % max_mean)\n",
    "print('**************************** Best Desc = ', best_desc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 3: Suggest improvements\n",
    "\n",
    "As you can observe, the clustering performance is poorer with 10 instruments. Using Essentia, you will implement the two different improvements described in the introduction of this assignment:\n",
    "- Better and more features: Shortlist a set of descriptors based on the sound characteristics of the instruments such that they can differentiate between the instruments. The choice of the descriptors computed is up to you. We suggest you compute many different descriptors similar to the ones returned by Freesound API, and additional ones described in the class lectures. The descriptors you used in A9 (but now computed using Essentia) are a good starting point. You can use the Essentia extractors that compute many frame-wise low level descrip- tors together (http://essentia.upf.edu/documentation/algorithms\\_overview.html# extractors)You can then use a subset of them for clustering for an improved clustering performance.\n",
    "- Computing the descriptors stripping the silences and noise at the beginning/end: For each sound, compute the energy of each frame of audio. You can then detect the low energy frames (silence) using a threshold on the energy of the frame. Since most of the single notes you will use are well recorded, the energy of silence regions is very low and a single threshold might work well for all the sounds. Plot the frame energy over time for a few sounds to determine a meaningful energy threshold. Subsequently, compute the mean descriptor value discarding these silent frames.\n",
    "\n",
    "Report the set of descriptors you computed and the performance it achieves, along with a brief explanation of your observations. You can also report the results for several combinations of features and finally report the best performance you achieved. Upload the code for computing the non-silent regions and for computing the descriptors that you used. Apart from the two enhancements suggested above, you are free to try further enhancements that improve clustering performance. In your report, describe these enhancements and the improvement they resulted in.\n",
    "\n",
    "Please upload now the code. You will be evaluated on the code you upload, and the observations and discussion in your report."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
